{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "max_iter = 5000\n",
    "#max_iter = 50#00 # DEBUG\n",
    "test_size = .25\n",
    "test_sizes = np.linspace(.15, .5, 6) # used to test robustness\n",
    "C = 1.\n",
    "Cs = np.geomspace(C/1000, C*10, 21) # used to test robustness\n",
    "multi_class = 'ovr'\n",
    "multi_class = 'multinomial'\n",
    "opts_LR = dict(max_iter=max_iter, multi_class=multi_class, penalty='l2', n_jobs=-1)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(p / (1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits import axes_grid1\n",
    "\n",
    "from matplotlib import colors as mcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "if not sys.platform in ['darwin', 'linux']:\n",
    "    plt.rcParams['animation.ffmpeg_path'] ='C:\\\\Users\\\\fucko\\\\Downloads\\\\ffmpeg\\\\bin\\\\ffmpeg.exe'  # TODO: use https://pypi.org/project/moviepy/ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(0, np.pi, 12, endpoint = False)    \n",
    "B_thetas = np.linspace(np.pi/2, 0 , 8) / 2.5\n",
    "colors = plt.cm.inferno(np.linspace(.8, .2, len(B_thetas))) #tc colormap\n",
    "\n",
    "exp_list = ['Mary', 'Tom', 'Steven']\n",
    "grouping_path = '../experimental/results/testgroup21092020/clusters/'\n",
    "\n",
    "# exp_idx = 1\n",
    "try :\n",
    "    cluster_list = np.load('./data/cluster_list.npy', allow_pickle = True)    \n",
    "except:    \n",
    "    cluster_list = [name for name in os.listdir(grouping_path) if os.path.isdir(grouping_path+name)]\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(cluster_list)\n",
    "\n",
    "    np.save('./data/cluster_list.npy', cluster_list)\n",
    "\n",
    "    \n",
    "# TODO :test with\n",
    "#rng = np.random.default_rng(1606)\n",
    "#rng.shuffle(cluster_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(f'{len(cluster_list)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_thetas = len(thetas)\n",
    "N_B_thetas = len(B_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = np.arange(-.2, .4, .01) #s\n",
    "t_start = 0.0\n",
    "t_stop = 0.3\n",
    "timebins = len(timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6653345369377348e-16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_timesteps = [-.15, .0, .3]\n",
    "cm_bt = [B_thetas[-1], B_thetas[4], B_thetas[0]]\n",
    "\n",
    "win_size = .1 #s\n",
    "win_sizes = np.linspace(.05, .2, 6)\n",
    "\n",
    "n_splits = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : which is the good one?\n",
    "rbins, abins = np.linspace(0.1, B_thetas[0], N_B_thetas+1)[::-1], np.linspace(0, np.pi, N_thetas+1)\n",
    "#rbins, abins = np.linspace(0.1, B_thetas[0], N_B_thetas+1), np.linspace(0, np.pi, N_thetas+1)\n",
    "EL, AZ = np.meshgrid(rbins, abins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62831853, 0.56227871, 0.4962389 , 0.43019908, 0.36415927,\n",
       "       0.29811945, 0.23207963, 0.16603982, 0.1       ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common functions regrouped in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use font params for Adobe Illustrator\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_minus(a) :\n",
    "    return 2 * ((a - np.min(a)) / (np.max(a) - np.min(a))) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_neuron_data(i, max_t, min_t,\n",
    "                     target_btheta,\n",
    "                     target_theta,\n",
    "                     data_type,\n",
    "                     cluster_list,\n",
    "                     fs = 30000. # frequency of sampling for recordings\n",
    "                    ) :\n",
    "    \n",
    "    cluster_path = cluster_list[i]\n",
    "    \n",
    "    spiketimes = np.load(grouping_path + cluster_path + '/spiketimes.npy')\n",
    "    spiketimes = spiketimes / fs\n",
    "    seq_contents = np.load(grouping_path + cluster_path + '/sequences_contents.npy', allow_pickle = True)\n",
    "\n",
    "    if data_type == 'one_bt':\n",
    "        spikes = [len(np.where((spiketimes > (seq['sequence_beg'] / fs) + min_t) &\n",
    "                    (spiketimes < (seq['sequence_beg'] / fs) + max_t))[0])\n",
    "                     for seq in seq_contents\n",
    "                     if seq['sequence_btheta'] == target_btheta]\n",
    "\n",
    "    elif data_type == 'bt_decoding_one_t' :\n",
    "        spikes = [len(np.where((spiketimes > (seq['sequence_beg'] / fs) + min_t) &\n",
    "                    (spiketimes < (seq['sequence_beg'] / fs) + max_t))[0])\n",
    "                     for seq in seq_contents\n",
    "                     if seq['sequence_theta'] == target_theta]\n",
    "\n",
    "    elif data_type in ['all_bt',  'all_t_bt', 'bt_decoding']:\n",
    "        spikes = [len(np.where((spiketimes > (seq['sequence_beg'] / fs) + min_t) &\n",
    "                    (spiketimes < (seq['sequence_beg'] / fs) + max_t))[0])\n",
    "                     for seq in seq_contents]\n",
    "        \n",
    "    else :\n",
    "        print('Data loading mode not implemented !')\n",
    "                \n",
    "    return spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def par_load_temporal_data(timesteps, target_btheta,\n",
    "                           target_theta, data_type, cluster_list,\n",
    "                           win_size=win_size,\n",
    "                           disable_tqdm = False) :\n",
    "    data = []\n",
    "    for timestep in tqdm(timesteps, desc = 'Loading data', disable=disable_tqdm) :\n",
    "        max_t = timestep + win_size\n",
    "        min_t = timestep\n",
    "\n",
    "        spikes = Parallel(n_jobs = -1)(delayed(load_neuron_data)(i,\n",
    "                                                       target_btheta = target_btheta,\n",
    "                                                       target_theta = target_theta,\n",
    "                                                       data_type = data_type,\n",
    "                                                       max_t = max_t,\n",
    "                                                       min_t = min_t,\n",
    "                                                       cluster_list = cluster_list)\n",
    "                             for i in range(len(cluster_list)))\n",
    "        data.append(spikes)\n",
    "\n",
    "    data = np.asarray(data)\n",
    "    data = np.swapaxes(data, 1, -1)\n",
    "    \n",
    "    seq_contents = np.load(grouping_path + cluster_list[0] + '/sequences_contents.npy', allow_pickle = True)\n",
    "    \n",
    "    if data_type == 'one_bt':\n",
    "        labels = ['T%.3f'% seq['sequence_theta'] for seq in seq_contents\n",
    "                 if seq['sequence_btheta'] == target_btheta]\n",
    "\n",
    "    elif data_type == 'all_bt':\n",
    "        labels = ['T%.3f'% seq['sequence_theta'] for seq in seq_contents]\n",
    "\n",
    "    elif data_type == 'all_t_bt':\n",
    "        labels = ['BT%.3fT%.3f'%(seq['sequence_btheta'],seq['sequence_theta']) \n",
    "                  for seq in seq_contents]\n",
    "\n",
    "    elif data_type == 'bt_decoding' :\n",
    "        labels = ['BT%.3f'% seq['sequence_btheta'] for seq in seq_contents]\n",
    "\n",
    "    elif data_type == 'bt_decoding_one_t' :\n",
    "        labels = ['BT%.3f'% seq['sequence_btheta'] for seq in seq_contents\n",
    "                  if seq['sequence_theta'] == target_theta]\n",
    "    \n",
    "    # Encode target labels with value between 0 and n_classes-1.\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html?highlight=labelencoder#sklearn.preprocessing.LabelEncoder \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels)\n",
    "\n",
    "    return data, le.transform(labels), le"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cluster_list[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    data = np.load('./data/data_all_bt.npy')\n",
    "    labels = np.load('./data/labels_all_bt.npy')\n",
    "except:    \n",
    "    # Data\n",
    "    data, labels, le = par_load_temporal_data(timesteps = timesteps, target_btheta = None,\n",
    "                                              target_theta = None, data_type = 'all_bt',\n",
    "                                              cluster_list = cluster_list)\n",
    "    np.save('./data/data_all_bt.npy', data)\n",
    "    np.save('./data/labels_all_bt.npy', labels)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    data = np.load('./data/data_all_t_bt.npy')\n",
    "    labels = np.load('./data/labels_all_t_bt.npy')\n",
    "except:    \n",
    "    # Data\n",
    "    data, labels, le = par_load_temporal_data(timesteps = timesteps, target_btheta = None,\n",
    "                                              target_theta = None, data_type = 'all_t_bt',\n",
    "                                              cluster_list = cluster_list)\n",
    "    np.save('./data/data_all_t_bt.npy', data)\n",
    "    np.save('./data/labels_all_t_bt.npy', labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence_beg': 103755.0,\n",
       " 'sequence_end': 117254.0,\n",
       " 'sequence_theta': 0.7853981633974483,\n",
       " 'sequence_btheta': 0.08975979010256552,\n",
       " 'sequence_phase': 0.025,\n",
       " 'stimtype': 'MC',\n",
       " 'spiketimes': array([[109182],\n",
       "        [109299],\n",
       "        [111028],\n",
       "        [111072],\n",
       "        [111141],\n",
       "        [111687],\n",
       "        [112530],\n",
       "        [112581],\n",
       "        [112691],\n",
       "        [112783]], dtype=uint64),\n",
       " 'tot_spikes': 10}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_contents = np.load(grouping_path + cluster_list[0] + '/sequences_contents.npy', allow_pickle = True)\n",
    "seq_contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['BT%.3fT%.3f'%(seq['sequence_btheta'],seq['sequence_theta']) \n",
    "                  for seq in seq_contents]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "i_labels = le.transform(labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(labels), len(i_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.unique(i_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i_label = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "for i_label in np.unique(i_labels):\n",
    "    print([labels[_] for _ in i_labels if _==i_label])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i_label in np.unique(i_labels):\n",
    "    print(i_label, [labels[_] for _ in i_labels if _==i_label][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BT0.000T0.000', 'BT0.000T0.262', 'BT0.000T0.524', 'BT0.000T0.785',\n",
       "       'BT0.000T1.047', 'BT0.000T1.309', 'BT0.000T1.571', 'BT0.000T1.833',\n",
       "       'BT0.000T2.094', 'BT0.000T2.356', 'BT0.000T2.618', 'BT0.000T2.880',\n",
       "       'BT0.090T0.000', 'BT0.090T0.262', 'BT0.090T0.524', 'BT0.090T0.785',\n",
       "       'BT0.090T1.047', 'BT0.090T1.309', 'BT0.090T1.571', 'BT0.090T1.833',\n",
       "       'BT0.090T2.094', 'BT0.090T2.356', 'BT0.090T2.618', 'BT0.090T2.880',\n",
       "       'BT0.180T0.000', 'BT0.180T0.262', 'BT0.180T0.524', 'BT0.180T0.785',\n",
       "       'BT0.180T1.047', 'BT0.180T1.309', 'BT0.180T1.571', 'BT0.180T1.833',\n",
       "       'BT0.180T2.094', 'BT0.180T2.356', 'BT0.180T2.618', 'BT0.180T2.880',\n",
       "       'BT0.269T0.000', 'BT0.269T0.262', 'BT0.269T0.524', 'BT0.269T0.785',\n",
       "       'BT0.269T1.047', 'BT0.269T1.309', 'BT0.269T1.571', 'BT0.269T1.833',\n",
       "       'BT0.269T2.094', 'BT0.269T2.356', 'BT0.269T2.618', 'BT0.269T2.880',\n",
       "       'BT0.359T0.000', 'BT0.359T0.262', 'BT0.359T0.524', 'BT0.359T0.785',\n",
       "       'BT0.359T1.047', 'BT0.359T1.309', 'BT0.359T1.571', 'BT0.359T1.833',\n",
       "       'BT0.359T2.094', 'BT0.359T2.356', 'BT0.359T2.618', 'BT0.359T2.880',\n",
       "       'BT0.449T0.000', 'BT0.449T0.262', 'BT0.449T0.524', 'BT0.449T0.785',\n",
       "       'BT0.449T1.047', 'BT0.449T1.309', 'BT0.449T1.571', 'BT0.449T1.833',\n",
       "       'BT0.449T2.094', 'BT0.449T2.356', 'BT0.449T2.618', 'BT0.449T2.880',\n",
       "       'BT0.539T0.000', 'BT0.539T0.262', 'BT0.539T0.524', 'BT0.539T0.785',\n",
       "       'BT0.539T1.047', 'BT0.539T1.309', 'BT0.539T1.571', 'BT0.539T1.833',\n",
       "       'BT0.539T2.094', 'BT0.539T2.356', 'BT0.539T2.618', 'BT0.539T2.880',\n",
       "       'BT0.628T0.000', 'BT0.628T0.262', 'BT0.628T0.524', 'BT0.628T0.785',\n",
       "       'BT0.628T1.047', 'BT0.628T1.309', 'BT0.628T1.571', 'BT0.628T1.833',\n",
       "       'BT0.628T2.094', 'BT0.628T2.356', 'BT0.628T2.618', 'BT0.628T2.880'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_ # rangé dans l'ordre alphabétique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BT0.000T0.000'], dtype='<U13')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm_kfold(cm_timesteps=cm_timesteps, win_size=win_size, cm_bt=cm_bt, n_splits=n_splits,\n",
    "                 shuffled = False, data_type = 'one_bt'):\n",
    "    all_cms = np.zeros((len(cm_bt),len(cm_timesteps), n_splits), dtype = object)\n",
    "    for ibt, bt in enumerate(cm_bt) :\n",
    "        data, labels, le = par_load_temporal_data(timesteps = cm_timesteps, target_btheta = bt,\n",
    "                                                target_theta = None, data_type=data_type,\n",
    "                                                cluster_list = cluster_list, win_size=win_size)\n",
    "\n",
    "        for i, t in enumerate(cm_timesteps):\n",
    "            d = data[i,:,:]\n",
    "            if shuffled :\n",
    "                np.random.seed(42)\n",
    "                np.random.shuffle(d)\n",
    "            kf = KFold(n_splits = n_splits)\n",
    "\n",
    "            for i1, (train_index, test_index) in enumerate(kf.split(d)):\n",
    "                xtrain, xtest = d[train_index], d[test_index]\n",
    "                ytrain, ytest = labels[train_index], labels[test_index]\n",
    "\n",
    "                logreg = LogisticRegression(**opts_LR)\n",
    "                logreg.fit(xtrain, ytrain)\n",
    "\n",
    "                cm = metrics.confusion_matrix(ytest, logreg.predict(xtest), normalize = 'all')\n",
    "                cm *= len(le.classes_)\n",
    "\n",
    "                all_cms[ibt, i, i1] = cm\n",
    "    return all_cms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://rasbt.github.io/mlxtend/user_guide/evaluate/permutation_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import permutation_test\n",
    "num_rounds = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_cm(cm1, cm2, \n",
    "             mshape=N_thetas, \n",
    "             num_rounds=num_rounds, \n",
    "             p_chance=1/N_thetas, n_splits=n_splits):\n",
    "    #all_w_mats = np.zeros((len(cm_bt),len(cm_timesteps)), dtype = object) #score\n",
    "    all_p_mats = np.zeros((len(cm_bt),len(cm_timesteps)), dtype = object) #pval\n",
    "    for ibt, _ in enumerate(cm_bt) :\n",
    "        for it, __ in enumerate(cm_timesteps):\n",
    "\n",
    "            #wil_mats = np.zeros((mshape, mshape))\n",
    "            p_mats = np.zeros((mshape, mshape))\n",
    "            for i1 in range(mshape) :\n",
    "                for i2 in range(mshape) :\n",
    "\n",
    "                    cm_pix = []\n",
    "                    for i3 in range(n_splits) :\n",
    "                        pix = all_cms[ibt, it, i3][i1, i2]\n",
    "                        cm_pix.append(pix)\n",
    "\n",
    "                    #s_cm_pix = [] \n",
    "                    #for i3 in range(n_splits) :\n",
    "                    #    pix = s_all_cms[ibt, it, i3][i1, i2]\n",
    "                    #    s_cm_pix.append(pix)\n",
    "\n",
    "                    p = permutation_test(cm_pix, np.full_like(cm_pix, p_chance), \n",
    "                                         num_rounds=num_rounds)\n",
    "                    #wil_mats[i1, i2] = 0\n",
    "                    p_mats[i1, i2] = p\n",
    "\n",
    "            #all_w_mats[ibt, it] = wil_mats\n",
    "            all_p_mats[ibt, it] = p_mats\n",
    "    \n",
    "    #return all_w_mats, all_p_mats\n",
    "    return all_p_mats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM validation, bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm_kfold_bt(cm_timesteps=cm_timesteps, win_size=win_size, n_splits=n_splits,\n",
    "                    shuffled = False):\n",
    "    all_cms = np.zeros((len(cm_timesteps), n_splits), dtype = object)\n",
    "    \n",
    "    data, labels, le = par_load_temporal_data(timesteps = cm_timesteps, target_btheta = None,\n",
    "                                            target_theta = None, data_type = 'bt_decoding',\n",
    "                                            cluster_list = cluster_list, win_size=win_size)\n",
    "\n",
    "    for i, t in enumerate(cm_timesteps):\n",
    "        d = data[i,:,:]\n",
    "        if shuffled :\n",
    "                np.random.seed(42)\n",
    "                np.random.shuffle(d)\n",
    "        kf = KFold(n_splits = n_splits)\n",
    "\n",
    "        for i1, (train_index, test_index) in enumerate(kf.split(d)):\n",
    "            xtrain, xtest = d[train_index], d[test_index]\n",
    "            ytrain, ytest = labels[train_index], labels[test_index]\n",
    "\n",
    "            logreg = LogisticRegression(**opts_LR)\n",
    "            logreg.fit(xtrain, ytrain)\n",
    "\n",
    "            cm = metrics.confusion_matrix(ytest, logreg.predict(xtest), normalize = 'all')\n",
    "            cm *= len(le.classes_)\n",
    "\n",
    "            all_cms[i, i1] = cm\n",
    "    return all_cms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_cm_bt(cm1, cm2, mshape=N_B_thetas, \n",
    "                num_rounds=num_rounds, p_chance=1./N_B_thetas, n_splits=n_splits):\n",
    "    #all_w_mats = np.zeros(len(cm_timesteps), dtype = object) #score\n",
    "    all_p_mats = np.zeros(len(cm_timesteps), dtype = object) #pval\n",
    "    for it, __ in enumerate(cm_timesteps):\n",
    "\n",
    "        #wil_mats = np.zeros((mshape, mshape))\n",
    "        p_mats = np.zeros((mshape, mshape))\n",
    "        for i1 in range(mshape) :\n",
    "            for i2 in range(mshape) :\n",
    "\n",
    "                cm_pix = []\n",
    "                for i3 in range(n_splits) :\n",
    "                    pix = all_cms[it, i3][i1, i2]\n",
    "                    cm_pix.append(pix)\n",
    "\n",
    "                #s_cm_pix = [] \n",
    "                #for i3 in range(n_splits) :\n",
    "                #    pix = s_all_cms[it, i3][i1, i2]\n",
    "                #    s_cm_pix.append(pix)\n",
    "\n",
    "                p = permutation_test(cm_pix, np.full_like(cm_pix, p_chance),\n",
    "                                     num_rounds = num_rounds)\n",
    "                #wil_mats[i1, i2] = 0\n",
    "                p_mats[i1, i2] = p\n",
    "\n",
    "        #all_w_mats[it] = wil_mats\n",
    "        all_p_mats[it] = p_mats\n",
    "    \n",
    "    #return all_w_mats, all_p_mats\n",
    "    return all_p_mats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cm validation, btt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm_kfold_btt(cm_timesteps=cm_timesteps, win_size=win_size, n_splits=n_splits,\n",
    "                     shuffled = False):\n",
    "    all_cms = np.zeros((len(cm_timesteps), n_splits), dtype = object)\n",
    "    \n",
    "    data, labels, le = par_load_temporal_data(timesteps = cm_timesteps, target_btheta = None,\n",
    "                                            target_theta = None, data_type = 'all_t_bt',\n",
    "                                            cluster_list = cluster_list, win_size=win_size)\n",
    "\n",
    "    for i, t in enumerate(cm_timesteps):\n",
    "        d = data[i,:,:]\n",
    "        \n",
    "        if shuffled :\n",
    "                np.random.seed(42)\n",
    "                np.random.shuffle(d)\n",
    "        kf = KFold(n_splits = n_splits)\n",
    "\n",
    "        for i1, (train_index, test_index) in enumerate(kf.split(d, labels)):\n",
    "            xtrain, xtest = d[train_index], d[test_index]\n",
    "            ytrain, ytest = labels[train_index], labels[test_index]\n",
    "\n",
    "            logreg = LogisticRegression(**opts_LR)\n",
    "            logreg.fit(xtrain, ytrain)\n",
    "\n",
    "            cm = metrics.confusion_matrix(ytest, logreg.predict(xtest), normalize = 'all')\n",
    "            cm *= len(le.classes_)\n",
    "\n",
    "            all_cms[i, i1] = cm\n",
    "    return all_cms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stats_cm_btt(cm1, cm2, mshape=N_thetas*N_B_thetas, \n",
    "                 num_rounds = num_rounds, \n",
    "                 p_chance=1/(N_thetas*N_B_thetas), n_splits=n_splits):\n",
    "    #all_w_mats = np.zeros(len(cm_timesteps), dtype = object) #score\n",
    "    all_p_mats = np.zeros(len(cm_timesteps), dtype = object) #pval\n",
    "    for it, __ in enumerate(cm_timesteps):\n",
    "\n",
    "        #wil_mats = np.zeros((mshape, mshape))\n",
    "        p_mats = np.zeros((mshape, mshape))\n",
    "        for i1 in range(mshape) :\n",
    "            for i2 in range(mshape) :\n",
    "\n",
    "                cm_pix = []\n",
    "                for i3 in range(n_splits) :\n",
    "                    pix = all_cms[it, i3][i1, i2]\n",
    "                    cm_pix.append(pix)\n",
    "\n",
    "                #s_cm_pix = [] \n",
    "                #for i3 in range(n_splits) :\n",
    "                #    pix = s_all_cms[it, i3][i1, i2]\n",
    "                #    s_cm_pix.append(pix)\n",
    "\n",
    "                p = permutation_test(cm_pix, np.full_like(cm_pix, p_chance),\n",
    "                                     num_rounds = num_rounds)\n",
    "                #wil_mats[i1, i2] = 0 # TODO : je comprends pas -> c'est toujours zero tout le temps\n",
    "                # On s'en sert plus, c'était des structures de données pour Wilcoxon\n",
    "                p_mats[i1, i2] = p\n",
    "\n",
    "        #all_w_mats[it] = wil_mats\n",
    "        all_p_mats[it] = p_mats\n",
    "    \n",
    "    return all_p_mats\n",
    "    # return all_w_mats, all_p_mats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cm plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pval_edges(arr, ax, lw = 3, c = 'k') :\n",
    "    #https://stackoverflow.com/questions/40892203/can-matplotlib-contours-match-pixel-edges\n",
    "    arr = arr.astype(float)\n",
    "\n",
    "    image = np.zeros(shape=(arr.shape[0]+2, arr.shape[1]+2))\n",
    "    image[1:-1, 1:-1] = arr\n",
    "\n",
    "    f = lambda x,y: image[int(y),int(x) ]\n",
    "    g = np.vectorize(f)\n",
    "\n",
    "    x = np.linspace(0,image.shape[1], image.shape[1]*100)\n",
    "    y = np.linspace(0,image.shape[0], image.shape[0]*100)\n",
    "    X, Y= np.meshgrid(x[:-1],y[:-1])\n",
    "    Z = g(X[:-1],Y[:-1])\n",
    "    Z = np.rot90(Z)\n",
    "    #plt.imshow(image[::-1], origin=\"lower\", interpolation=\"none\", cmap=\"Blues\")\n",
    "    Z = np.roll(Z, 100, axis = 0)\n",
    "    Z = np.roll(Z, -100, axis = 1)\n",
    "    ax.contour(Z[::-1], [0.5], colors=c, linewidths=[lw], \n",
    "                extent=[0-0.5, x[:-1].max()-0.5,0-0.5, y[:-1].max()-0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "\n",
    "def timer(func):\n",
    "    \"\"\"Print the runtime of the decorated function\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_timer(*args, **kwargs):\n",
    "        start_time = time.perf_counter()    # 1\n",
    "        results = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()      # 2\n",
    "        run_time = end_time - start_time    # 3\n",
    "        print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\n",
    "        return results\n",
    "    return wrapper_timer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
